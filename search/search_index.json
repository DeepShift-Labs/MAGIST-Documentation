{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":true,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MAGIST-Algorithm Multi-Agent Generally Intelligent Simultaneous Training Algorithm for Project Zeta! Official Documentation We now have official documentation available here ! Working Principle Data The data is the most crucial element as the entire intelligence works around it. That is why the AI needs to process it and provide reasonable assumptions. There is another condition, however: the algorithm, in its finished state, MUST be strictly Python code. This means no pre-trained models or presets. It must find its data and process it unsupervised. Although this architecture seems hard-coded in the present state, as more functionality is added, it will be more intelligent and decisive. Here, the data will assume the following structure: Object -> Common Associated Verbs, Synonyms, Events, Timestamps of usage, Nearby Objects, etc. This is the \"Who, What, When, Where, Why, How\" of the data. This data can later be filtered and called upon when inferences are needed. To get here, however, the data must first be extracted from a single image. Since no pre-trained models are allowed, here is the process to follow: 1. K-Means Clustering(find key objects in the image) -> Discriminator for integrity check(see if clustering was performed well) 2. Reverse Image Search and Google Scraping(find the label of image) -> Data Downloader(find dataset from large datasets) 3. Transfer Learn Model -> Object Detector 4. Get a summary from internet sources like Wikipedia; location from LiDAR(future); user from facial detection(future); etc. 5. Store all the data in the NeuralDB Natural Language Processing Another key stage of this AGI(Artificial General Intelligence) is human interaction and understanding. MAGIST will constantly listen to conversations and make intelligent decisions. Here is the target process: 1. Record audio data and transcribe it(this is ***the only*** place where a pre-trained model(the transcriber) is used since learning a human language fully unsupervised is incredibly arduous. 2. Use a custom positional embedding with a Self-Attention head to find keywords. 3. Search these terms in the NeuralDB for possible entries. 4. Search unknown terms online and store definitions for future reference. (future) 5. Extract key terms from matching entries. (future) 6. Insert those key terms into a text transformer trained on the collected NeuralDB data to generate a prediction. (future) 7. Utter the prediction. (future) Usage This project is still under development. Please contact me at krishna.shah@deepshift.dev if you want immediate access and/or support to MAGIST. Once the algorithm is in a stable state, I will release a Python Package on PyPI and Github for access. There will also be documentation with more instructions. Installation This project has many dependencies. Most can be installed using pip . Some require OS-level package managers. This is going to work best in Linux-based systems. These instructions are for Linux-based systems. In particular for Ubuntu 20.04 LTS based operating systems. Other systems may have errors that will require debugging. Linux (Ubuntu-based Systems) First, install Python 3 and pip : sudo apt install python3 python3-dev python3-pip Next, we need to install Firefox and its corresponding geckodriver for headless Selenium searches: sudo apt install firefox Note: If you get an error regarding the geckodriver, you can install it manually by following the instructions here . Install System Packages Next, we need to install the system packages that MAGIST uses. sudo apt install python3-pyaudio sudo apt install libasound-dev MongoDB Next, we need to install MongoDB. This is a database that MAGIST uses to store its data. Please go to the MongoDB Website and follow the instructions to install it. We have more instructions in the documentation . Next, create a Python environment. There are 2 ways to do this: Anaconda or VEnv. Anaconda First, install Anaconda from https://www.anaconda.com/. Make the Anaconda environment: conda create --name myenv Activate the environment in your current console. Note: You will have to do this every time you want to run MAGIST. conda activate myenv Get the latest MAGIST Wheel from our releases page ! Install MAGIST: pip3 install MAGIST-Algorithm-x.x.x-py3-none-any.whl VEnv Make the environment in a designated location. python3 -m venv /path/to/new/virtual/environment To activate it, you must travel to that path/bin/ and then run: source activate Get the latest MAGIST Wheel from our releases page ! Install MAGIST: pip3 install MAGIST-Algorithm-x.x.x-py3-none-any.whl Congratulations! You are all set up to script and use MAGIST Contributing Your contribution, monetary or programmatically, is crucial for the rapid development of the algorithm and its training. Please consider contributing. Even minute changes to our README will be greatly appreciated. Project Zeta We are building a fully biomimetic robot dog to implement MAGIST into. This will serve as the gateway between MAGIST and the physical world. Disclaimer Artificial Intelligence is a powerful field meant only for research and study , and it should be kept that way. The unethical use of MAGIST can have severe repercussions for society and the perpetrator. DeepShift Labs and any programs it develops are strictly for research purposes. Hence, all of our products, MAGIST included, are to be used strictly for research purposes. Misuse of this program can lead to heavy fines and prosecution. Furthermore, to retitle, rebrand, or redistribute without explicitly crediting DeepShift Labs is illegal.","title":"Read Me"},{"location":"#magist-algorithm","text":"Multi-Agent Generally Intelligent Simultaneous Training Algorithm for Project Zeta!","title":"MAGIST-Algorithm"},{"location":"#official-documentation","text":"We now have official documentation available here !","title":"Official Documentation"},{"location":"#working-principle","text":"","title":"Working Principle"},{"location":"#data","text":"The data is the most crucial element as the entire intelligence works around it. That is why the AI needs to process it and provide reasonable assumptions. There is another condition, however: the algorithm, in its finished state, MUST be strictly Python code. This means no pre-trained models or presets. It must find its data and process it unsupervised. Although this architecture seems hard-coded in the present state, as more functionality is added, it will be more intelligent and decisive. Here, the data will assume the following structure: Object -> Common Associated Verbs, Synonyms, Events, Timestamps of usage, Nearby Objects, etc. This is the \"Who, What, When, Where, Why, How\" of the data. This data can later be filtered and called upon when inferences are needed. To get here, however, the data must first be extracted from a single image. Since no pre-trained models are allowed, here is the process to follow: 1. K-Means Clustering(find key objects in the image) -> Discriminator for integrity check(see if clustering was performed well) 2. Reverse Image Search and Google Scraping(find the label of image) -> Data Downloader(find dataset from large datasets) 3. Transfer Learn Model -> Object Detector 4. Get a summary from internet sources like Wikipedia; location from LiDAR(future); user from facial detection(future); etc. 5. Store all the data in the NeuralDB","title":"Data"},{"location":"#natural-language-processing","text":"Another key stage of this AGI(Artificial General Intelligence) is human interaction and understanding. MAGIST will constantly listen to conversations and make intelligent decisions. Here is the target process: 1. Record audio data and transcribe it(this is ***the only*** place where a pre-trained model(the transcriber) is used since learning a human language fully unsupervised is incredibly arduous. 2. Use a custom positional embedding with a Self-Attention head to find keywords. 3. Search these terms in the NeuralDB for possible entries. 4. Search unknown terms online and store definitions for future reference. (future) 5. Extract key terms from matching entries. (future) 6. Insert those key terms into a text transformer trained on the collected NeuralDB data to generate a prediction. (future) 7. Utter the prediction. (future)","title":"Natural Language Processing"},{"location":"#usage","text":"This project is still under development. Please contact me at krishna.shah@deepshift.dev if you want immediate access and/or support to MAGIST. Once the algorithm is in a stable state, I will release a Python Package on PyPI and Github for access. There will also be documentation with more instructions.","title":"Usage"},{"location":"#installation","text":"This project has many dependencies. Most can be installed using pip . Some require OS-level package managers. This is going to work best in Linux-based systems. These instructions are for Linux-based systems. In particular for Ubuntu 20.04 LTS based operating systems. Other systems may have errors that will require debugging.","title":"Installation"},{"location":"#linux-ubuntu-based-systems","text":"First, install Python 3 and pip : sudo apt install python3 python3-dev python3-pip Next, we need to install Firefox and its corresponding geckodriver for headless Selenium searches: sudo apt install firefox Note: If you get an error regarding the geckodriver, you can install it manually by following the instructions here .","title":"Linux (Ubuntu-based Systems)"},{"location":"#install-system-packages","text":"Next, we need to install the system packages that MAGIST uses. sudo apt install python3-pyaudio sudo apt install libasound-dev","title":"Install System Packages"},{"location":"#mongodb","text":"Next, we need to install MongoDB. This is a database that MAGIST uses to store its data. Please go to the MongoDB Website and follow the instructions to install it. We have more instructions in the documentation . Next, create a Python environment. There are 2 ways to do this: Anaconda or VEnv.","title":"MongoDB"},{"location":"#anaconda","text":"First, install Anaconda from https://www.anaconda.com/. Make the Anaconda environment: conda create --name myenv Activate the environment in your current console. Note: You will have to do this every time you want to run MAGIST. conda activate myenv Get the latest MAGIST Wheel from our releases page ! Install MAGIST: pip3 install MAGIST-Algorithm-x.x.x-py3-none-any.whl","title":"Anaconda"},{"location":"#venv","text":"Make the environment in a designated location. python3 -m venv /path/to/new/virtual/environment To activate it, you must travel to that path/bin/ and then run: source activate Get the latest MAGIST Wheel from our releases page ! Install MAGIST: pip3 install MAGIST-Algorithm-x.x.x-py3-none-any.whl Congratulations! You are all set up to script and use MAGIST","title":"VEnv"},{"location":"#contributing","text":"Your contribution, monetary or programmatically, is crucial for the rapid development of the algorithm and its training. Please consider contributing. Even minute changes to our README will be greatly appreciated.","title":"Contributing"},{"location":"#project-zeta","text":"We are building a fully biomimetic robot dog to implement MAGIST into. This will serve as the gateway between MAGIST and the physical world.","title":"Project Zeta"},{"location":"#disclaimer","text":"Artificial Intelligence is a powerful field meant only for research and study , and it should be kept that way. The unethical use of MAGIST can have severe repercussions for society and the perpetrator. DeepShift Labs and any programs it develops are strictly for research purposes. Hence, all of our products, MAGIST included, are to be used strictly for research purposes. Misuse of this program can lead to heavy fines and prosecution. Furthermore, to retitle, rebrand, or redistribute without explicitly crediting DeepShift Labs is illegal.","title":"Disclaimer"},{"location":"Configuration/1%20-%20Setup%20Config%20File/","text":"Modular Configuration System (MCS) MAGIST uses a custom configuration system that we call MCS. MCS is a modular method for managing data-flows and MAGIST subcomponents. For example, for the dataflow \"images\", we have the \"MAGIST Vision\" which contains the \"DetectionDataManager\", \"FullySupervisedModels\", and \"SemiSupervisedModels\". Although all subcomponents do not require configuration, having such categorization provides the modularity that makes MAGIST community-friendly. If community members are interested in making custom modules using the base MAGIST API, they can readily do that through this configuration and a consistent module setup that will be discussed in later wiki pages. Line-By-Line Sample Config Explanation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 { \"api_authentication\" : [ // (1) { \"google\" : [ { \"api_key\" : \"***\" }, { \"project_cx\" : \"***\" }, { \"GIS_downloader_verbose\" : 0 } ]} ], \"system_administration\" : [ // (2) { \"sudo_password\" : \"***\" } ], \"task_management\" : [ // (3) { \"num_of_worker_threads\" : 4 } ], \"paths\" : [ // (4) { \"log_dir\" : \"Logs\" } ], \"basic_variables\" : [ // (5) { \"verbose\" : 1 }, { \"enable_matplot_display\" : 0 } ], \"tf_lite_detector\" : [ // (6) { \"data_path\" : \"Data\" }, { \"TensorBoard_log_dir\" : \"Tensorflow/TensorBoard\" }, { \"TF_ckpt_path\" : \"Tensorflow/tf_ckpt\" }, { \"input_image_size\" : [ 50 , 50 ]}, { \"grayscale\" : 0 }, { \"epochs\" : 10 }, { \"batch_size\" : 32 }, { \"seed\" : 42 }, { \"validation_split\" : 0.2 }, { \"export_full_model\" : \"Tensorflow/Full_TF_Model\" } ], \"neural_db\" : [ // (7) (8) { \"mongo_socket\" : \"mongodb://localhost:27017/\" }, //(9) { \"db_search_zone\" : [ \"vision\" , \"nlp\" , \"common\" ]} // (10) ] } This is a large container to manage all API related authentication and configuration. This can contain subsets by authentication method or API provider. This is primarily meant for system-level commands with elevated privileges. 3rd party modules will rarely use this subcomponent. This subcomponent manages all the tasks, threads, cores, and other system resources for faster and more distributed operation. These are global paths for MAGIST core system. 3rd parties will NOT use this subcomponent and will define paths in their own module as the \"tf_lite_detector\" module did. This component manages verbose and debug. 3rd parties will likely use this as a global verbose setting for their subcomponents. This is our first actual subcomponent. This is what 3rd parties will be creating and implementing into the existing MAGIST system. This can contain other elements regarding their module like paths, modes, parameters, etc. This is another example of a subcomponent of MAGIST implemented into the configuration. This comment was specifically added since this functionality is only available for MAGIST with MongoDB. Newer versions deprecated MongoDB and instead use ElasticSearch. This comment was specifically added since this functionality is only available for MAGIST with MongoDB. Newer versions deprecated MongoDB and instead use ElasticSearch. This comment was specifically added since this functionality is only available for MAGIST with MongoDB. Newer versions deprecated MongoDB and instead use ElasticSearch. Note Please note that this config.json is a sample and the exact parameters vary based on the environment. Other Important Configuration Files The file explored in the previous section is the config.json file under the src/config folder on our repository. There are other files that pertain to this project that must also be explored: - config * config.json * queries.json * schema.json * schema_nested.json Note schema_nested.json is an experimental file that used ElasticSearch nested databases. This file should be ignored. This wiki will only cover schema.json and queries.json since they are relevant to the project. Queries Configuration After the deprecation of MongoDB, MAGIST uses ElasticSearch to perform data storage, management, and querying. This is a drastic change from the first stable release (v0.1.0) which used a completely custom nested querying method. Due to the high-performance search functions the ElasticSearch API exposes, MAGIST transitioned to using them. ElasticSearch entirely runs on JSON queries which is why we created separate configuration files for them. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"object_exists\" : { // (1) \"query\" : { \"query_string\" : { \"query\" : \"\" , \"fields\" : [ \"name\" ], // (2) \"analyzer\" : \"keyword\" // (3) } } }, \"object_full\" : { // (4) \"query\" : { \"multi_match\" : { \"query\" : \"\" , // (5) \"analyzer\" : \"main_analyzer\" // (6) } } }, \"word_exists\" : { // (7) \"query\" : { \"query_string\" : { \"query\" : \"\" , \"fields\" : [ \"name\" ], \"analyzer\" : \"keyword\" } } }, \"word_full\" : { // (8) \"query\" : { \"multi_match\" : { \"query\" : \"\" , \"analyzer\" : \"main_analyzer\" } } } } Query to check if an object exists given a keyword. This does a more narrow search as opposed to the full database search that the following query does. Sets the fields to search through to \"name\" which is just the object name. Runs a keyword search which is meant for names. Query to do a complete database search for objects by any datapoint: name, description, location, usage, etc. Sets the field to wildcard. Uses a custom analyzer to effectively search the database for information. Functions identically to object_exists , but for words. Functions identically to object_full , but for words. These queries can be modified to suit user preferences. Warning MAGIST will simply use the JSON dictionary contained in the categories ( object_exists , object_full , word_exists , word_full ) EXACTLY as they are written here. This means that these dictionaries are fed DIRECTLY into ElasticSearch. Please use ElasticSearch configuration style, keywords, and arguments when modifying. Data Schema Configuration The schema.json file is responsible for defining search parameters, ElasticSearch settings, and the database schema. The schema will structure the data in a logical, easy to search format that can easily be expanded. Adding new modules or datatypes will require extensive modification of this schema. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 { \"object_db_schema\" : { // (1) \"settings\" : { // (2) \"analysis\" : { // (3) \"analyzer\" : { \"main_analyzer\" : { // (4) \"type\" : \"fingerprint\" , // (5) \"stopwords\" : \"_english_\" // (6) } } }, \"similarity\" : { // (7) \"main_similarity\" : { \"type\" : \"LMJelinekMercer\" , // (8) \"lambda\" : 0.5 // (9) } } }, \"mappings\" : { // (10) \"properties\" : { // (11) \"name\" : { \"type\" : \"keyword\" }, // (12) \"description\" : { \"type\" : \"text\" }, // (13) \"users\" : { \"type\" : \"text\" }, // (14) \"related_objects\" : { \"type\" : \"text\" }, // (15) \"location\" : { \"type\" : \"text\" } // (16) } } }, \"word_db_schema\" : { // (17) \"settings\" : { \"analysis\" : { \"analyzer\" : { \"main_analyzer\" : { \"type\" : \"fingerprint\" , \"stopwords\" : \"_english_\" } } }, \"similarity\" : { \"main_similarity\" : { \"type\" : \"LMJelinekMercer\" , \"lambda\" : 0.5 } } }, \"mappings\" : { \"properties\" : { \"word\" : { \"type\" : \"text\" }, // (18) \"definition\" : { \"type\" : \"text\" }, // (19) \"users\" : { \"type\" : \"text\" }, // (20) \"related_words\" : { \"type\" : \"text\" }, // (21) \"related_objects\" : { \"type\" : \"text\" }, // (22) \"location\" : { \"type\" : \"text\" } // (23) } } } } The container for the object database schema. This is the ElasticSearch database settings that defines the analyzer and search settings. The analyzer is a set of algorithms that define the exact semantics on how a word/sentence/paragraph is to be interpreted, searched, and processed. This defines the custom analyzer that MAGIST uses. The analyzer uses the fingerprint algorithm will \"lowercase, normalize to remove extended characters, sort, deduplicate and concatenate into a single token. If a stop-word list is configured, stop words will also be removed\" 1 . Defines the dictionary to use when deleting stop-words like \"is\", \"the\", \"an\", etc. This defines the similarity algorithm ElasticSearch will used when evaluating the similarity of the query to the data sample. This algorithm will \"attempt to capture important patterns in the text, while leaving out noise\" 2 . The lambda parameter is one of the requirements for the algorithm to function. More about LM Jelinek Mercer lambda parameter This defines the database schema (a.k.a mapping) that the database will use to store each document (this refers to a single datapoint with several attributes). In this case, properties refers to the attributes of the object itself and are not ElasticSearch. Name of the objects Description of the object scraped from online sources. People who have been recognized to be using that object. Other objects that relate to this object. Where the object was found. Same database schema container as the object_db_schema but for words. The word that is being stored. The definition of the word. People who were found to use that word. Other words that relate to this one. Objects that relate to the word. Where the word was used. Warning MAGIST will simply use the JSON dictionary contained in the categories ( object_db_schema , word_db_schema ) EXACTLY as they are written here. This means that these dictionaries are fed DIRECTLY into ElasticSearch. Please use ElasticSearch configuration style, keywords, and arguments when modifying. This quote is directly taken from the ElasticSearch documentation regarding the fingerprint algorithm. Link to the page \u21a9 This quote is directly taken from the ElasticSearch documentation regarding the LM Jelinek Mercer algorithm. More about LM Jelinek Mercer \u21a9","title":"Setup Config File"},{"location":"Configuration/1%20-%20Setup%20Config%20File/#modular-configuration-system-mcs","text":"MAGIST uses a custom configuration system that we call MCS. MCS is a modular method for managing data-flows and MAGIST subcomponents. For example, for the dataflow \"images\", we have the \"MAGIST Vision\" which contains the \"DetectionDataManager\", \"FullySupervisedModels\", and \"SemiSupervisedModels\". Although all subcomponents do not require configuration, having such categorization provides the modularity that makes MAGIST community-friendly. If community members are interested in making custom modules using the base MAGIST API, they can readily do that through this configuration and a consistent module setup that will be discussed in later wiki pages.","title":"Modular Configuration System (MCS)"},{"location":"Configuration/1%20-%20Setup%20Config%20File/#line-by-line-sample-config-explanation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 { \"api_authentication\" : [ // (1) { \"google\" : [ { \"api_key\" : \"***\" }, { \"project_cx\" : \"***\" }, { \"GIS_downloader_verbose\" : 0 } ]} ], \"system_administration\" : [ // (2) { \"sudo_password\" : \"***\" } ], \"task_management\" : [ // (3) { \"num_of_worker_threads\" : 4 } ], \"paths\" : [ // (4) { \"log_dir\" : \"Logs\" } ], \"basic_variables\" : [ // (5) { \"verbose\" : 1 }, { \"enable_matplot_display\" : 0 } ], \"tf_lite_detector\" : [ // (6) { \"data_path\" : \"Data\" }, { \"TensorBoard_log_dir\" : \"Tensorflow/TensorBoard\" }, { \"TF_ckpt_path\" : \"Tensorflow/tf_ckpt\" }, { \"input_image_size\" : [ 50 , 50 ]}, { \"grayscale\" : 0 }, { \"epochs\" : 10 }, { \"batch_size\" : 32 }, { \"seed\" : 42 }, { \"validation_split\" : 0.2 }, { \"export_full_model\" : \"Tensorflow/Full_TF_Model\" } ], \"neural_db\" : [ // (7) (8) { \"mongo_socket\" : \"mongodb://localhost:27017/\" }, //(9) { \"db_search_zone\" : [ \"vision\" , \"nlp\" , \"common\" ]} // (10) ] } This is a large container to manage all API related authentication and configuration. This can contain subsets by authentication method or API provider. This is primarily meant for system-level commands with elevated privileges. 3rd party modules will rarely use this subcomponent. This subcomponent manages all the tasks, threads, cores, and other system resources for faster and more distributed operation. These are global paths for MAGIST core system. 3rd parties will NOT use this subcomponent and will define paths in their own module as the \"tf_lite_detector\" module did. This component manages verbose and debug. 3rd parties will likely use this as a global verbose setting for their subcomponents. This is our first actual subcomponent. This is what 3rd parties will be creating and implementing into the existing MAGIST system. This can contain other elements regarding their module like paths, modes, parameters, etc. This is another example of a subcomponent of MAGIST implemented into the configuration. This comment was specifically added since this functionality is only available for MAGIST with MongoDB. Newer versions deprecated MongoDB and instead use ElasticSearch. This comment was specifically added since this functionality is only available for MAGIST with MongoDB. Newer versions deprecated MongoDB and instead use ElasticSearch. This comment was specifically added since this functionality is only available for MAGIST with MongoDB. Newer versions deprecated MongoDB and instead use ElasticSearch. Note Please note that this config.json is a sample and the exact parameters vary based on the environment.","title":"Line-By-Line Sample Config Explanation"},{"location":"Configuration/1%20-%20Setup%20Config%20File/#other-important-configuration-files","text":"The file explored in the previous section is the config.json file under the src/config folder on our repository. There are other files that pertain to this project that must also be explored: - config * config.json * queries.json * schema.json * schema_nested.json Note schema_nested.json is an experimental file that used ElasticSearch nested databases. This file should be ignored. This wiki will only cover schema.json and queries.json since they are relevant to the project.","title":"Other Important Configuration Files"},{"location":"Configuration/1%20-%20Setup%20Config%20File/#queries-configuration","text":"After the deprecation of MongoDB, MAGIST uses ElasticSearch to perform data storage, management, and querying. This is a drastic change from the first stable release (v0.1.0) which used a completely custom nested querying method. Due to the high-performance search functions the ElasticSearch API exposes, MAGIST transitioned to using them. ElasticSearch entirely runs on JSON queries which is why we created separate configuration files for them. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"object_exists\" : { // (1) \"query\" : { \"query_string\" : { \"query\" : \"\" , \"fields\" : [ \"name\" ], // (2) \"analyzer\" : \"keyword\" // (3) } } }, \"object_full\" : { // (4) \"query\" : { \"multi_match\" : { \"query\" : \"\" , // (5) \"analyzer\" : \"main_analyzer\" // (6) } } }, \"word_exists\" : { // (7) \"query\" : { \"query_string\" : { \"query\" : \"\" , \"fields\" : [ \"name\" ], \"analyzer\" : \"keyword\" } } }, \"word_full\" : { // (8) \"query\" : { \"multi_match\" : { \"query\" : \"\" , \"analyzer\" : \"main_analyzer\" } } } } Query to check if an object exists given a keyword. This does a more narrow search as opposed to the full database search that the following query does. Sets the fields to search through to \"name\" which is just the object name. Runs a keyword search which is meant for names. Query to do a complete database search for objects by any datapoint: name, description, location, usage, etc. Sets the field to wildcard. Uses a custom analyzer to effectively search the database for information. Functions identically to object_exists , but for words. Functions identically to object_full , but for words. These queries can be modified to suit user preferences. Warning MAGIST will simply use the JSON dictionary contained in the categories ( object_exists , object_full , word_exists , word_full ) EXACTLY as they are written here. This means that these dictionaries are fed DIRECTLY into ElasticSearch. Please use ElasticSearch configuration style, keywords, and arguments when modifying.","title":"Queries Configuration"},{"location":"Configuration/1%20-%20Setup%20Config%20File/#data-schema-configuration","text":"The schema.json file is responsible for defining search parameters, ElasticSearch settings, and the database schema. The schema will structure the data in a logical, easy to search format that can easily be expanded. Adding new modules or datatypes will require extensive modification of this schema. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 { \"object_db_schema\" : { // (1) \"settings\" : { // (2) \"analysis\" : { // (3) \"analyzer\" : { \"main_analyzer\" : { // (4) \"type\" : \"fingerprint\" , // (5) \"stopwords\" : \"_english_\" // (6) } } }, \"similarity\" : { // (7) \"main_similarity\" : { \"type\" : \"LMJelinekMercer\" , // (8) \"lambda\" : 0.5 // (9) } } }, \"mappings\" : { // (10) \"properties\" : { // (11) \"name\" : { \"type\" : \"keyword\" }, // (12) \"description\" : { \"type\" : \"text\" }, // (13) \"users\" : { \"type\" : \"text\" }, // (14) \"related_objects\" : { \"type\" : \"text\" }, // (15) \"location\" : { \"type\" : \"text\" } // (16) } } }, \"word_db_schema\" : { // (17) \"settings\" : { \"analysis\" : { \"analyzer\" : { \"main_analyzer\" : { \"type\" : \"fingerprint\" , \"stopwords\" : \"_english_\" } } }, \"similarity\" : { \"main_similarity\" : { \"type\" : \"LMJelinekMercer\" , \"lambda\" : 0.5 } } }, \"mappings\" : { \"properties\" : { \"word\" : { \"type\" : \"text\" }, // (18) \"definition\" : { \"type\" : \"text\" }, // (19) \"users\" : { \"type\" : \"text\" }, // (20) \"related_words\" : { \"type\" : \"text\" }, // (21) \"related_objects\" : { \"type\" : \"text\" }, // (22) \"location\" : { \"type\" : \"text\" } // (23) } } } } The container for the object database schema. This is the ElasticSearch database settings that defines the analyzer and search settings. The analyzer is a set of algorithms that define the exact semantics on how a word/sentence/paragraph is to be interpreted, searched, and processed. This defines the custom analyzer that MAGIST uses. The analyzer uses the fingerprint algorithm will \"lowercase, normalize to remove extended characters, sort, deduplicate and concatenate into a single token. If a stop-word list is configured, stop words will also be removed\" 1 . Defines the dictionary to use when deleting stop-words like \"is\", \"the\", \"an\", etc. This defines the similarity algorithm ElasticSearch will used when evaluating the similarity of the query to the data sample. This algorithm will \"attempt to capture important patterns in the text, while leaving out noise\" 2 . The lambda parameter is one of the requirements for the algorithm to function. More about LM Jelinek Mercer lambda parameter This defines the database schema (a.k.a mapping) that the database will use to store each document (this refers to a single datapoint with several attributes). In this case, properties refers to the attributes of the object itself and are not ElasticSearch. Name of the objects Description of the object scraped from online sources. People who have been recognized to be using that object. Other objects that relate to this object. Where the object was found. Same database schema container as the object_db_schema but for words. The word that is being stored. The definition of the word. People who were found to use that word. Other words that relate to this one. Objects that relate to the word. Where the word was used. Warning MAGIST will simply use the JSON dictionary contained in the categories ( object_db_schema , word_db_schema ) EXACTLY as they are written here. This means that these dictionaries are fed DIRECTLY into ElasticSearch. Please use ElasticSearch configuration style, keywords, and arguments when modifying. This quote is directly taken from the ElasticSearch documentation regarding the fingerprint algorithm. Link to the page \u21a9 This quote is directly taken from the ElasticSearch documentation regarding the LM Jelinek Mercer algorithm. More about LM Jelinek Mercer \u21a9","title":"Data Schema Configuration"},{"location":"Getting%20Started/1%20-%20Initial%20Steps/","text":"Welcome We are thrilled that you have shown interest in the MAGIST project. We are currently in the process of building the MAGIST. As a result, there might be issues or bugs that you encounter. If you encounter any issues, please post an issue on the MAGIST GitHub repository. We will try to resolve them as soon as possible. If you have a solution to the issue, you may create a pull request, and we will review it as soon as possible. With that out of the way, let's get started! How it works? We are using a powerful technique that combine the decades of research our brilliant researchers have done. We are using a multi-agent approach to intelligently process real-world data and learn from it. To understand this, however, it is crucial to comprehend how we define general intelligence, as there are major discrepancies from source to source. Our Goal General intelligence is the ability of a machine to automatically acquire and train itself on a vast variety of knowledge that it may acquire from any source. It must also be able to make new predictions on the data upon inquisition (direct or indirect). This machine may not start with any pre-trained models, data, etc. Setup MAGIST is a complex project, so a clean and efficient work space is essential. The entirety of this project was made in the following environment: PyCharm Professional Python 3.10.4 Anaconda Environment Firefox Headless Replicating this environment is crucial for stability and performance. Complete Installation Guide This project has many dependencies. Most can be installed using pip . Some require OS-level package managers. These instruction are for Linux-based systems. In particular for Ubuntu 20.04 LTS based operating systems. Other systems may have errors that will require debugging. Linux (Ubuntu-based Systems) First install Python 3 and pip : sudo apt python3 python3-dev python3-pip Next, we need to install Firefox and its corresponding geckodriver for headless Selenium searches: sudo apt install firefox Note: If you get an error regarding the geckodriver, you can install it manually by following the instructions here . Install System Packages Next, we need to install the system packages that MAGIST uses. sudo apt install python3-pyaudio sudo apt install libasound-dev MongoDB Next, we need to install MongoDB. This is a database that MAGIST uses to store its data. wget -qO - https://www.mongodb.org/static/pgp/server-5.0.asc | sudo apt-key add - If you get an error with the command above, you need to install gnupg and then re-import the key. sudo apt install gnupg wget -qO - https://www.mongodb.org/static/pgp/server-5.0.asc | sudo apt-key add - Next, we need to make the list file: echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/5.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-5.0.list Reload the package list: sudo apt update Finally install MongoDB: sudo apt install mongodb-org Next, create a Python environment. There are 2 ways to do this: Anaconda or VEnv. Anaconda First install Anaconda from https://www.anaconda.com/. You will need to download the latest installer and then run the following commands: sudo chmod +x Anaconda3-xxxx.xx-Linux-x86_64.sh ./Anaconda3-xxxx.xx-Linux-x86_64.sh Close and reopen all terminal windows. Make the Anaconda environment: conda create --name myenv Activate the environment in your current console. Note: You will have to do this every time you want to run MAGIST. conda activate myenv Install MAGIST: pip3 install MAGIST-Algorithm-x.x.x-py3-none-any.whl VEnv Make the environment in a designated location. python3 -m venv /path/to/new/virtual/environment To activate it, you must travel to that path/bin/ and then run: source activate Install MAGIST: pip3 install MAGIST-Algorithm-x.x.x-py3-none-any.whl Congratulations! You are all setup to script and use MAGIST Enterprise Setup Future versions of MAGIST will come with ElasticSearch as a powerful search alternative to MongoDB. The setup is a lot harder involving a separate server and more powerful computer hardware. A future section of this Wiki will cover Enterprise setup.","title":"Installation"},{"location":"Getting%20Started/1%20-%20Initial%20Steps/#welcome","text":"We are thrilled that you have shown interest in the MAGIST project. We are currently in the process of building the MAGIST. As a result, there might be issues or bugs that you encounter. If you encounter any issues, please post an issue on the MAGIST GitHub repository. We will try to resolve them as soon as possible. If you have a solution to the issue, you may create a pull request, and we will review it as soon as possible. With that out of the way, let's get started!","title":"Welcome"},{"location":"Getting%20Started/1%20-%20Initial%20Steps/#how-it-works","text":"We are using a powerful technique that combine the decades of research our brilliant researchers have done. We are using a multi-agent approach to intelligently process real-world data and learn from it. To understand this, however, it is crucial to comprehend how we define general intelligence, as there are major discrepancies from source to source.","title":"How it works?"},{"location":"Getting%20Started/1%20-%20Initial%20Steps/#our-goal","text":"General intelligence is the ability of a machine to automatically acquire and train itself on a vast variety of knowledge that it may acquire from any source. It must also be able to make new predictions on the data upon inquisition (direct or indirect). This machine may not start with any pre-trained models, data, etc.","title":"Our Goal"},{"location":"Getting%20Started/1%20-%20Initial%20Steps/#setup","text":"MAGIST is a complex project, so a clean and efficient work space is essential. The entirety of this project was made in the following environment: PyCharm Professional Python 3.10.4 Anaconda Environment Firefox Headless Replicating this environment is crucial for stability and performance.","title":"Setup"},{"location":"Getting%20Started/1%20-%20Initial%20Steps/#complete-installation-guide","text":"This project has many dependencies. Most can be installed using pip . Some require OS-level package managers. These instruction are for Linux-based systems. In particular for Ubuntu 20.04 LTS based operating systems. Other systems may have errors that will require debugging.","title":"Complete Installation Guide"},{"location":"Getting%20Started/1%20-%20Initial%20Steps/#linux-ubuntu-based-systems","text":"First install Python 3 and pip : sudo apt python3 python3-dev python3-pip Next, we need to install Firefox and its corresponding geckodriver for headless Selenium searches: sudo apt install firefox Note: If you get an error regarding the geckodriver, you can install it manually by following the instructions here .","title":"Linux (Ubuntu-based Systems)"},{"location":"Getting%20Started/1%20-%20Initial%20Steps/#install-system-packages","text":"Next, we need to install the system packages that MAGIST uses. sudo apt install python3-pyaudio sudo apt install libasound-dev","title":"Install System Packages"},{"location":"Getting%20Started/1%20-%20Initial%20Steps/#mongodb","text":"Next, we need to install MongoDB. This is a database that MAGIST uses to store its data. wget -qO - https://www.mongodb.org/static/pgp/server-5.0.asc | sudo apt-key add - If you get an error with the command above, you need to install gnupg and then re-import the key. sudo apt install gnupg wget -qO - https://www.mongodb.org/static/pgp/server-5.0.asc | sudo apt-key add - Next, we need to make the list file: echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/5.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-5.0.list Reload the package list: sudo apt update Finally install MongoDB: sudo apt install mongodb-org Next, create a Python environment. There are 2 ways to do this: Anaconda or VEnv.","title":"MongoDB"},{"location":"Getting%20Started/1%20-%20Initial%20Steps/#anaconda","text":"First install Anaconda from https://www.anaconda.com/. You will need to download the latest installer and then run the following commands: sudo chmod +x Anaconda3-xxxx.xx-Linux-x86_64.sh ./Anaconda3-xxxx.xx-Linux-x86_64.sh Close and reopen all terminal windows. Make the Anaconda environment: conda create --name myenv Activate the environment in your current console. Note: You will have to do this every time you want to run MAGIST. conda activate myenv Install MAGIST: pip3 install MAGIST-Algorithm-x.x.x-py3-none-any.whl","title":"Anaconda"},{"location":"Getting%20Started/1%20-%20Initial%20Steps/#venv","text":"Make the environment in a designated location. python3 -m venv /path/to/new/virtual/environment To activate it, you must travel to that path/bin/ and then run: source activate Install MAGIST: pip3 install MAGIST-Algorithm-x.x.x-py3-none-any.whl Congratulations! You are all setup to script and use MAGIST","title":"VEnv"},{"location":"Getting%20Started/1%20-%20Initial%20Steps/#enterprise-setup","text":"Future versions of MAGIST will come with ElasticSearch as a powerful search alternative to MongoDB. The setup is a lot harder involving a separate server and more powerful computer hardware. A future section of this Wiki will cover Enterprise setup.","title":"Enterprise Setup"},{"location":"Getting%20Started/2%20-%20Process%20Overview/","text":"Process Overview Understanding the core fundamentals of MAGIST is crucial for designing a custom process pipeline using the powerful features the MAGIST API exposes. Current System Diagram This is the exact process running the current MAGIST process pipeline. There are hundreds of features left to implement. Note: We do NOT claim that this current process is MAGIST's final form OR that this current process is Generally Intelligent. With proper language processors, transformers, MDCs, etc., it has the potential of being the World's first AGI(Artificial General Intelligence). The part below the divider is going to change soon in later versions of MAGIST. Upcoming Features These features will be implemented after the divider shown above. * ElasticSearch Support for NeuralDB LSTM Gated RNN for Unique Response Synthesis Live Object-Detection and Transfer Learning Other data-types: LiDAR, Motors, etc. * Configurable modules/plug-ins. * How can this be an AGI? Human brains, or any animal brain, process information in logical, discrete steps. It takes the data, runs some pre-processing, and then computes an environment. This is our perception of reality. In fact, if humans believe hard enough, it is possible to influence this computation and see a different reality. With MAGIST, we have taken a similar approach. The data is processed, in turn, slowly extracting meaning from it. This contradicts most modern powerful AI models that use a single, massive network. Some examples include GPT-3, BERT, Imagen, etc. This limits the functionality of the AI, in terms of AGIs, since human-procured data has to be fed to it. This data is sorted and assessed with a fine-toothed comb and removes the stochasticity the environment contains. This also reduces its ability to teach itself new tasks. Despite being multi-purpose, it is functionally limited. That is why MAGIST uses a multi-agent, self-supervised approach. This will allow it to teach itself a lot of new things simply by following the outlined pipeline. Although MAGIST is currently limited in the types of data it can process and predict, it can evolve to be incredibly intelligent.","title":"Process Overview"},{"location":"Getting%20Started/2%20-%20Process%20Overview/#process-overview","text":"Understanding the core fundamentals of MAGIST is crucial for designing a custom process pipeline using the powerful features the MAGIST API exposes.","title":"Process Overview"},{"location":"Getting%20Started/2%20-%20Process%20Overview/#current-system-diagram","text":"This is the exact process running the current MAGIST process pipeline. There are hundreds of features left to implement. Note: We do NOT claim that this current process is MAGIST's final form OR that this current process is Generally Intelligent. With proper language processors, transformers, MDCs, etc., it has the potential of being the World's first AGI(Artificial General Intelligence). The part below the divider is going to change soon in later versions of MAGIST.","title":"Current System Diagram"},{"location":"Getting%20Started/2%20-%20Process%20Overview/#upcoming-features","text":"These features will be implemented after the divider shown above. * ElasticSearch Support for NeuralDB LSTM Gated RNN for Unique Response Synthesis Live Object-Detection and Transfer Learning Other data-types: LiDAR, Motors, etc. * Configurable modules/plug-ins. *","title":"Upcoming Features"},{"location":"Getting%20Started/2%20-%20Process%20Overview/#how-can-this-be-an-agi","text":"Human brains, or any animal brain, process information in logical, discrete steps. It takes the data, runs some pre-processing, and then computes an environment. This is our perception of reality. In fact, if humans believe hard enough, it is possible to influence this computation and see a different reality. With MAGIST, we have taken a similar approach. The data is processed, in turn, slowly extracting meaning from it. This contradicts most modern powerful AI models that use a single, massive network. Some examples include GPT-3, BERT, Imagen, etc. This limits the functionality of the AI, in terms of AGIs, since human-procured data has to be fed to it. This data is sorted and assessed with a fine-toothed comb and removes the stochasticity the environment contains. This also reduces its ability to teach itself new tasks. Despite being multi-purpose, it is functionally limited. That is why MAGIST uses a multi-agent, self-supervised approach. This will allow it to teach itself a lot of new things simply by following the outlined pipeline. Although MAGIST is currently limited in the types of data it can process and predict, it can evolve to be incredibly intelligent.","title":"How can this be an AGI?"},{"location":"Getting%20Started/3%20-%20Next%20Steps/","text":"Next Steps If the prospect of developing the world's first fully-functional general intelligence is as exciting for you as it is for me, please follow these next steps to properly understand the MAGIST API. From there, you can modify or rewrite the processing pipeline to suit your needs. There are many approaches to achieving General Intelligence, our pipeline is one of the simplest and most effective ones. Feel free to find new processes that also aim to develop an AGI. Contributing In the process of scripting using MAGIST API, if you find any bugs, issues, or have a feature request or idea, visit my GitHub Issues and report an issue. If you want to see the development in progress at DeepShift Labs, take a look at our GitHub Projects . Showing Support If you want to support for this project, please don't hesitate to visit these links: Website OpenCollective YouTube Need Help? If you need any assistance setting up or using MAGIST, please contact me .","title":"Next Steps"},{"location":"Getting%20Started/3%20-%20Next%20Steps/#next-steps","text":"If the prospect of developing the world's first fully-functional general intelligence is as exciting for you as it is for me, please follow these next steps to properly understand the MAGIST API. From there, you can modify or rewrite the processing pipeline to suit your needs. There are many approaches to achieving General Intelligence, our pipeline is one of the simplest and most effective ones. Feel free to find new processes that also aim to develop an AGI.","title":"Next Steps"},{"location":"Getting%20Started/3%20-%20Next%20Steps/#contributing","text":"In the process of scripting using MAGIST API, if you find any bugs, issues, or have a feature request or idea, visit my GitHub Issues and report an issue. If you want to see the development in progress at DeepShift Labs, take a look at our GitHub Projects .","title":"Contributing"},{"location":"Getting%20Started/3%20-%20Next%20Steps/#showing-support","text":"If you want to support for this project, please don't hesitate to visit these links: Website OpenCollective YouTube","title":"Showing Support"},{"location":"Getting%20Started/3%20-%20Next%20Steps/#need-help","text":"If you need any assistance setting up or using MAGIST, please contact me .","title":"Need Help?"},{"location":"MAGIST%20Vision/1%20-%20Basics/","text":"Basics MAGIST Vision is a paramount component of the algorithm that is comprised of powerful Convolutional Neural Networks, unsupervised clustering algorithms, and image processing utilities. This includes: MAGIST CNN Detector Image Processing Tools for MAGIST CNN K-Means Image Pixel Clustering","title":"Basics"},{"location":"MAGIST%20Vision/1%20-%20Basics/#basics","text":"MAGIST Vision is a paramount component of the algorithm that is comprised of powerful Convolutional Neural Networks, unsupervised clustering algorithms, and image processing utilities. This includes: MAGIST CNN Detector Image Processing Tools for MAGIST CNN K-Means Image Pixel Clustering","title":"Basics"},{"location":"MAGIST%20Vision/2%20-%20K-Means%20Clustering/","text":"K-Means Clustering Working Principle K-Means Clustering is a simple but powerful unsupervised learning algorithm. K-Means Clustering aims to divide a given sample data into \"k\" groups. The process is intuitively simple: Place k number centroids of the clusters in random locations in the data space. For each sample in the data space, associate its class with the nearest centroid. For each cluster, compute the geometric center and move the centroids there. Repeat steps 2 and 3 until you don't need to change the location of the centroid. Automatically compute \"k\" There is a way to find the number of clusters(k) you need. This technique utilizes the elbow curve that is generated by creating a line graph of the number of clusters \"k\" and the corresponding reduction in variation. This is not currently implemented in MAGIST, but there are plans to implement it soon. 1 1 Implementation Using MAGIST Currently, MAGIST uses K-Means Clustering for masking images. It attempts to identify individual objects in an image which can then be further processed to extract more information. For this, you must use the UnsupervisedModels under the Vision directory. It can be imported like so: from MAGIST.Vision.UnsupervisedModels.img_cluster import RoughCluster # (1) UnsupervisedModels contains a Python file named img_cluster.py that contains the RoughCluster class. This is where all the necessary methods for computing the clusters. Note This computation is done with the help of SciKit-Learn and Pandas. Now, create an instance of the RoughCluster class: clusterer = RoughCluster ( \"PATH/TO/CONFIG.json\" ) # (1) More information about the config file is located here: Setup Config File Finally, we can compute the cluster: n_of_clusters = 3 img_location = \"Data/test.jpg\" img_size = ( 200 , 200 ) masked_img_dir = \"Data/Clusters\" masked_img_locations = clusterer . unsupervised_clusters ( n_of_clusters , img_location , img_size , masked_img_dir ) Here is what masked_img_locations should look like: [ \"Data/Clusters/masked0.jpg\" , \"Data/Clusters/masked1.jpg\" , \"Data/Clusters/masked2.jpg\" ] # (1) The amount of images listed depends on the n_of_clusters . These images are the final masked results. Through the config file, you can enable verbose to see the intermediate stage before it individually exporting the images. These images were acquired from WikiPedia . \u21a9 \u21a9","title":"K-Means Clustering"},{"location":"MAGIST%20Vision/2%20-%20K-Means%20Clustering/#k-means-clustering","text":"","title":"K-Means Clustering"},{"location":"MAGIST%20Vision/2%20-%20K-Means%20Clustering/#working-principle","text":"K-Means Clustering is a simple but powerful unsupervised learning algorithm. K-Means Clustering aims to divide a given sample data into \"k\" groups. The process is intuitively simple: Place k number centroids of the clusters in random locations in the data space. For each sample in the data space, associate its class with the nearest centroid. For each cluster, compute the geometric center and move the centroids there. Repeat steps 2 and 3 until you don't need to change the location of the centroid. Automatically compute \"k\" There is a way to find the number of clusters(k) you need. This technique utilizes the elbow curve that is generated by creating a line graph of the number of clusters \"k\" and the corresponding reduction in variation. This is not currently implemented in MAGIST, but there are plans to implement it soon. 1 1","title":"Working Principle"},{"location":"MAGIST%20Vision/2%20-%20K-Means%20Clustering/#implementation-using-magist","text":"Currently, MAGIST uses K-Means Clustering for masking images. It attempts to identify individual objects in an image which can then be further processed to extract more information. For this, you must use the UnsupervisedModels under the Vision directory. It can be imported like so: from MAGIST.Vision.UnsupervisedModels.img_cluster import RoughCluster # (1) UnsupervisedModels contains a Python file named img_cluster.py that contains the RoughCluster class. This is where all the necessary methods for computing the clusters. Note This computation is done with the help of SciKit-Learn and Pandas. Now, create an instance of the RoughCluster class: clusterer = RoughCluster ( \"PATH/TO/CONFIG.json\" ) # (1) More information about the config file is located here: Setup Config File Finally, we can compute the cluster: n_of_clusters = 3 img_location = \"Data/test.jpg\" img_size = ( 200 , 200 ) masked_img_dir = \"Data/Clusters\" masked_img_locations = clusterer . unsupervised_clusters ( n_of_clusters , img_location , img_size , masked_img_dir ) Here is what masked_img_locations should look like: [ \"Data/Clusters/masked0.jpg\" , \"Data/Clusters/masked1.jpg\" , \"Data/Clusters/masked2.jpg\" ] # (1) The amount of images listed depends on the n_of_clusters . These images are the final masked results. Through the config file, you can enable verbose to see the intermediate stage before it individually exporting the images. These images were acquired from WikiPedia . \u21a9 \u21a9","title":"Implementation Using MAGIST"},{"location":"MAGIST%20Vision/MAGIST%20Object%20Detection/3%20-%20MAGIST%20Object%20Detection%20Basics/","text":"MAGIST Object Detection Basics MAGIST uses a multi-layer Convolutional Neural Network (CNN) to efficiently detect and frame relevant objects. The conventional methods to do object detection (not recognition) are as follows: YOLO --> \"You Look Only Once\" uses a combination of IOU (intersection over union), bounding box regression, and residual blocks. RCNN/Faster-RCNN --> Uses a region proposal network to create regions of interest and a CNN to find the region with the highest confidence of the predicted object to be entirely bounded. Due to the input data restrictions (imposed by the definition of general intelligence), these techniques are unusable. MAGIST runs an algorithm that is a combination of YOLO's grid system and a \"temperature\" based detection system for pose estimation. Working Principle MAGIST Object Detection works in 4 main phases: Splitting input images into smaller segments (follows grid-like shape with no overlapping regions) Training and predicting on each individual segment. Using a temperature threshold to highlight boxes of the highest accuracy. Compute bounding box from the highlighted images. Isolating the Foreground When using this technique, our data samples will also include the background as a correct as well as the object itself. The reason this poses a potential problem is that is can potentially teach the AI on incorrect, yet correctly marked. However, after training on several hundred thousand data samples, if the majority of the data is the object, the AI will \"favor\" the more common data and exclude the extraneous outliers.","title":"MAGIST Object Detection Basics"},{"location":"MAGIST%20Vision/MAGIST%20Object%20Detection/3%20-%20MAGIST%20Object%20Detection%20Basics/#magist-object-detection-basics","text":"MAGIST uses a multi-layer Convolutional Neural Network (CNN) to efficiently detect and frame relevant objects. The conventional methods to do object detection (not recognition) are as follows: YOLO --> \"You Look Only Once\" uses a combination of IOU (intersection over union), bounding box regression, and residual blocks. RCNN/Faster-RCNN --> Uses a region proposal network to create regions of interest and a CNN to find the region with the highest confidence of the predicted object to be entirely bounded. Due to the input data restrictions (imposed by the definition of general intelligence), these techniques are unusable. MAGIST runs an algorithm that is a combination of YOLO's grid system and a \"temperature\" based detection system for pose estimation.","title":"MAGIST Object Detection Basics"},{"location":"MAGIST%20Vision/MAGIST%20Object%20Detection/3%20-%20MAGIST%20Object%20Detection%20Basics/#working-principle","text":"MAGIST Object Detection works in 4 main phases: Splitting input images into smaller segments (follows grid-like shape with no overlapping regions) Training and predicting on each individual segment. Using a temperature threshold to highlight boxes of the highest accuracy. Compute bounding box from the highlighted images.","title":"Working Principle"},{"location":"MAGIST%20Vision/MAGIST%20Object%20Detection/3%20-%20MAGIST%20Object%20Detection%20Basics/#isolating-the-foreground","text":"When using this technique, our data samples will also include the background as a correct as well as the object itself. The reason this poses a potential problem is that is can potentially teach the AI on incorrect, yet correctly marked. However, after training on several hundred thousand data samples, if the majority of the data is the object, the AI will \"favor\" the more common data and exclude the extraneous outliers.","title":"Isolating the Foreground"},{"location":"MAGIST%20Vision/MAGIST%20Object%20Detection/4%20-%20Detection%20Data%20Management/","text":"Detection Data Management This component of MAGIST deals with image data manipulation and processing.","title":"Detection Data Management"},{"location":"MAGIST%20Vision/MAGIST%20Object%20Detection/4%20-%20Detection%20Data%20Management/#detection-data-management","text":"This component of MAGIST deals with image data manipulation and processing.","title":"Detection Data Management"}]}